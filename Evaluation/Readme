Evaluation (Inference_testing.ipynb):

  -	By using the evaluation sets extracted and preprocessed previously (tweets_test_formal_preprocessed.csv, tweets_test_informal_preprocessed.csv) we predict the label for each instance of them using for each both models:
    the ones trained in DistilBERT_NoNE.ipynb and DistilBERT_wNE.ipynb respectively

  -	Through these 4 possible combinations (formal with NE, formal NE free, informal with NE, informal NE free) we calculate the Precision, Recall and F1 Score for each by comparing the actual label with the prediction
    using the functions from sklearn to do this.



PyPremise (Pypremise.ipynb): 
	
  -	To run this notebook, it is necessary to provide the proper Dataset, already having Predictions and real Labels
  
  -	In this notebook we solely implemented the NE free predicted sets, due to them being less noisy and more interpretable

  -	PyPremise is a library that can be used for detecting patterns for the prediction outcomes provided.
    To get these patterns we use the predicted Datasets from evaluation (predicted_NO_NE_tweets_test_formal_preprocessed.csv, predicted_NO_NE_tweets_test_informal_preprocessed.csv) and count the matches (1)
    and mismatches (0) between predictions and real labels. We then save the tokenized words of each sentence in a list called features, while the PyPremise matching labels are saved in the Py_labels = [] list. 

  -	The outcome shows in which kind of features the model might show issues or perform specifically well.

